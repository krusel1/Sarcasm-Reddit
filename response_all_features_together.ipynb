{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verb</th>\n",
       "      <th>funct</th>\n",
       "      <th>auxverb</th>\n",
       "      <th>present</th>\n",
       "      <th>pronoun</th>\n",
       "      <th>ipron</th>\n",
       "      <th>cogmech</th>\n",
       "      <th>certain</th>\n",
       "      <th>ppron</th>\n",
       "      <th>you</th>\n",
       "      <th>social</th>\n",
       "      <th>affect</th>\n",
       "      <th>posemo</th>\n",
       "      <th>conj</th>\n",
       "      <th>tentat</th>\n",
       "      <th>excl</th>\n",
       "      <th>future</th>\n",
       "      <th>discrep</th>\n",
       "      <th>past</th>\n",
       "      <th>motion</th>\n",
       "      <th>relativ</th>\n",
       "      <th>space</th>\n",
       "      <th>preps</th>\n",
       "      <th>incl</th>\n",
       "      <th>adverb</th>\n",
       "      <th>insight</th>\n",
       "      <th>shehe</th>\n",
       "      <th>achieve</th>\n",
       "      <th>negemo</th>\n",
       "      <th>anger</th>\n",
       "      <th>they</th>\n",
       "      <th>cause</th>\n",
       "      <th>article</th>\n",
       "      <th>money</th>\n",
       "      <th>work</th>\n",
       "      <th>percept</th>\n",
       "      <th>hear</th>\n",
       "      <th>swear</th>\n",
       "      <th>time</th>\n",
       "      <th>negate</th>\n",
       "      <th>leisure</th>\n",
       "      <th>death</th>\n",
       "      <th>number</th>\n",
       "      <th>see</th>\n",
       "      <th>sad</th>\n",
       "      <th>quant</th>\n",
       "      <th>anx</th>\n",
       "      <th>assent</th>\n",
       "      <th>bio</th>\n",
       "      <th>health</th>\n",
       "      <th>filler</th>\n",
       "      <th>body</th>\n",
       "      <th>friend</th>\n",
       "      <th>sexual</th>\n",
       "      <th>we</th>\n",
       "      <th>home</th>\n",
       "      <th>feel</th>\n",
       "      <th>inhib</th>\n",
       "      <th>humans</th>\n",
       "      <th>i</th>\n",
       "      <th>family</th>\n",
       "      <th>relig</th>\n",
       "      <th>nonfl</th>\n",
       "      <th>ingest</th>\n",
       "      <th>label</th>\n",
       "      <th>noun_count_percent</th>\n",
       "      <th>adj_count_percent</th>\n",
       "      <th>adv_count_percent</th>\n",
       "      <th>pro_count_percent</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>upper_case_word_count</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>vader_neu</th>\n",
       "      <th>vader_compound</th>\n",
       "      <th>response</th>\n",
       "      <th>Response_emotion</th>\n",
       "      <th>valence_response</th>\n",
       "      <th>arousal_response</th>\n",
       "      <th>dominance_response</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SARCASM</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>4.862069</td>\n",
       "      <td>0.167658</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.87530</td>\n",
       "      <td>I don't get this .. obviously you do car...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>6.222500</td>\n",
       "      <td>3.695000</td>\n",
       "      <td>5.965000</td>\n",
       "      <td>['I', 'do', \"n't\", 'get', 'this', '..', 'obvio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SARCASM</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.11985</td>\n",
       "      <td>trying to protest about . Talking about hi...</td>\n",
       "      <td>anger</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>3.713333</td>\n",
       "      <td>5.733333</td>\n",
       "      <td>['trying', 'to', 'protest', 'about', '.', 'Tal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SARCASM</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>5.777778</td>\n",
       "      <td>0.320988</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.27130</td>\n",
       "      <td>He makes an insane about of money from t...</td>\n",
       "      <td>anger</td>\n",
       "      <td>5.246667</td>\n",
       "      <td>5.223333</td>\n",
       "      <td>5.426667</td>\n",
       "      <td>['He', 'makes', 'an', 'insane', 'about', 'of',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SARCASM</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>0.243056</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.24470</td>\n",
       "      <td>Meanwhile Trump won't even release his SAT...</td>\n",
       "      <td>anger</td>\n",
       "      <td>5.661111</td>\n",
       "      <td>4.428889</td>\n",
       "      <td>5.907778</td>\n",
       "      <td>['Meanwhile', 'Trump', 'wo', \"n't\", 'even', 'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SARCASM</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>5.575758</td>\n",
       "      <td>0.168962</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.83525</td>\n",
       "      <td>Pretty Sure the Anti-Lincoln Crowd Claimed...</td>\n",
       "      <td>joy</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>['Pretty', 'Sure', 'the', 'Anti-Lincoln', 'Cro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   verb  funct  auxverb  present  pronoun  ipron  cogmech  certain  ppron  \\\n",
       "0     7     13        3        5        5      1        6        1      4   \n",
       "1     2     11        1        2        5      1        3        0      4   \n",
       "2     1      5        0        1        0      0        1        0      0   \n",
       "3     3      9        1        0        4      0        2        1      4   \n",
       "4     0      5        0        0        0      0        0        0      0   \n",
       "\n",
       "   you  social  affect  posemo  conj  tentat  excl  future  discrep  past  \\\n",
       "0    3       4       2       2     2       1     1       1        1     1   \n",
       "1    0       4       1       0     2       0     0       0        0     0   \n",
       "2    0       0       0       0     0       0     0       0        0     0   \n",
       "3    0       5       1       0     1       0     0       0        0     3   \n",
       "4    0       0       0       0     0       0     0       0        0     0   \n",
       "\n",
       "   motion  relativ  space  preps  incl  adverb  insight  shehe  achieve  \\\n",
       "0       1        2      1      2     2       1        1      1        0   \n",
       "1       0        0      0      3     2       2        0      2        1   \n",
       "2       0        0      0      3     0       1        0      0        0   \n",
       "3       0        1      0      0     1       2        0      3        0   \n",
       "4       0        2      2      2     0       1        0      0        0   \n",
       "\n",
       "   negemo  anger  they  cause  article  money  work  percept  hear  swear  \\\n",
       "0       0      0     0      0        0      0     0        0     0      0   \n",
       "1       1      1     2      1        0      0     0        0     0      0   \n",
       "2       0      0     0      1        2      1     0        0     0      0   \n",
       "3       1      1     1      0        1      0     3        1     1      1   \n",
       "4       0      0     0      0        2      0     0        0     0      0   \n",
       "\n",
       "   time  negate  leisure  death  number  see  sad  quant  anx  assent  bio  \\\n",
       "0     0       0        0      0       0    0    0      0    0       0    0   \n",
       "1     0       0        0      0       0    0    0      0    0       0    0   \n",
       "2     0       0        0      0       0    0    0      0    0       0    0   \n",
       "3     1       0        0      0       0    0    0      0    0       0    0   \n",
       "4     0       0        0      0       0    0    0      0    0       0    0   \n",
       "\n",
       "   health  filler  body  friend  sexual  we  home  feel  inhib  humans  i  \\\n",
       "0       0       0     0       0       0   0     0     0      0       0  0   \n",
       "1       0       0     0       0       0   0     0     0      0       0  0   \n",
       "2       0       0     0       0       0   0     0     0      0       0  0   \n",
       "3       0       0     0       0       0   0     0     0      0       0  0   \n",
       "4       0       0     0       0       0   0     0     0      0       0  0   \n",
       "\n",
       "   family  relig  nonfl  ingest    label  noun_count_percent  \\\n",
       "0       0      0      0       0  SARCASM            0.310345   \n",
       "1       0      0      0       0  SARCASM            0.250000   \n",
       "2       0      0      0       0  SARCASM            0.055556   \n",
       "3       0      0      0       0  SARCASM            0.208333   \n",
       "4       0      0      0       0  SARCASM            0.060606   \n",
       "\n",
       "   adj_count_percent  adv_count_percent  pro_count_percent  char_count  \\\n",
       "0           0.034483           0.172414           0.172414    4.862069   \n",
       "1           0.000000           0.000000           0.166667    5.000000   \n",
       "2           0.000000           0.000000           0.055556    5.777778   \n",
       "3           0.041667           0.166667           0.166667    5.833333   \n",
       "4           0.030303           0.030303           0.030303    5.575758   \n",
       "\n",
       "   word_density  punctuation_count  upper_case_word_count  vader_pos  \\\n",
       "0      0.167658           0.379310               0.137931      0.204   \n",
       "1      0.208333           0.166667               0.125000      0.000   \n",
       "2      0.320988           0.333333               0.222222      0.000   \n",
       "3      0.243056           0.166667               0.125000      0.000   \n",
       "4      0.168962           0.393939               0.090909      0.193   \n",
       "\n",
       "   vader_neg  vader_neu  vader_compound  \\\n",
       "0      0.000      0.796         0.87530   \n",
       "1      0.256      0.744         0.11985   \n",
       "2      0.176      0.824         0.27130   \n",
       "3      0.130      0.870         0.24470   \n",
       "4      0.000      0.807         0.83525   \n",
       "\n",
       "                                            response Response_emotion  \\\n",
       "0        I don't get this .. obviously you do car...          sadness   \n",
       "1      trying to protest about . Talking about hi...            anger   \n",
       "2        He makes an insane about of money from t...            anger   \n",
       "3      Meanwhile Trump won't even release his SAT...            anger   \n",
       "4      Pretty Sure the Anti-Lincoln Crowd Claimed...              joy   \n",
       "\n",
       "   valence_response  arousal_response  dominance_response  \\\n",
       "0          6.222500          3.695000            5.965000   \n",
       "1          5.190000          3.713333            5.733333   \n",
       "2          5.246667          5.223333            5.426667   \n",
       "3          5.661111          4.428889            5.907778   \n",
       "4          5.000000          5.000000            5.000000   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  ['I', 'do', \"n't\", 'get', 'this', '..', 'obvio...  \n",
       "1  ['trying', 'to', 'protest', 'about', '.', 'Tal...  \n",
       "2  ['He', 'makes', 'an', 'insane', 'about', 'of',...  \n",
       "3  ['Meanwhile', 'Trump', 'wo', \"n't\", 'even', 'r...  \n",
       "4  ['Pretty', 'Sure', 'the', 'Anti-Lincoln', 'Cro...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TODO\n",
    "\n",
    "csv_file = '/Users/swcam/Documents/GitHub/Sarcasm/Final_data/liwc_response_train.csv'\n",
    "#csv_file = '/Users/swcam/Documents/GitHub/Sarcasm/Final_Features_train_data'\n",
    "Train_df = pd.read_csv(csv_file)\n",
    "Train_df = Train_df.loc[:, ~Train_df.columns.str.contains('^Unnamed')]\n",
    "# print out the first few rows of data info\n",
    "Train_df.head(5)\n",
    "\n",
    "#These are the most useful features per SHAP \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>relativ</th>\n",
       "      <th>funct</th>\n",
       "      <th>pronoun</th>\n",
       "      <th>ipron</th>\n",
       "      <th>adverb</th>\n",
       "      <th>cogmech</th>\n",
       "      <th>excl</th>\n",
       "      <th>leisure</th>\n",
       "      <th>conj</th>\n",
       "      <th>incl</th>\n",
       "      <th>verb</th>\n",
       "      <th>past</th>\n",
       "      <th>social</th>\n",
       "      <th>ppron</th>\n",
       "      <th>i</th>\n",
       "      <th>cause</th>\n",
       "      <th>humans</th>\n",
       "      <th>certain</th>\n",
       "      <th>achieve</th>\n",
       "      <th>preps</th>\n",
       "      <th>tentat</th>\n",
       "      <th>space</th>\n",
       "      <th>affect</th>\n",
       "      <th>filler</th>\n",
       "      <th>posemo</th>\n",
       "      <th>present</th>\n",
       "      <th>they</th>\n",
       "      <th>shehe</th>\n",
       "      <th>negemo</th>\n",
       "      <th>anger</th>\n",
       "      <th>quant</th>\n",
       "      <th>auxverb</th>\n",
       "      <th>article</th>\n",
       "      <th>insight</th>\n",
       "      <th>work</th>\n",
       "      <th>you</th>\n",
       "      <th>motion</th>\n",
       "      <th>discrep</th>\n",
       "      <th>assent</th>\n",
       "      <th>inhib</th>\n",
       "      <th>home</th>\n",
       "      <th>percept</th>\n",
       "      <th>hear</th>\n",
       "      <th>anx</th>\n",
       "      <th>sad</th>\n",
       "      <th>see</th>\n",
       "      <th>money</th>\n",
       "      <th>negate</th>\n",
       "      <th>bio</th>\n",
       "      <th>health</th>\n",
       "      <th>sexual</th>\n",
       "      <th>nonfl</th>\n",
       "      <th>future</th>\n",
       "      <th>swear</th>\n",
       "      <th>ingest</th>\n",
       "      <th>feel</th>\n",
       "      <th>number</th>\n",
       "      <th>body</th>\n",
       "      <th>relig</th>\n",
       "      <th>family</th>\n",
       "      <th>we</th>\n",
       "      <th>death</th>\n",
       "      <th>friend</th>\n",
       "      <th>label</th>\n",
       "      <th>noun_count_percent</th>\n",
       "      <th>adj_count_percent</th>\n",
       "      <th>adv_count_percent</th>\n",
       "      <th>pro_count_percent</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>upper_case_word_count</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>vader_neu</th>\n",
       "      <th>vader_compound</th>\n",
       "      <th>response</th>\n",
       "      <th>Response_emotion</th>\n",
       "      <th>valence_response</th>\n",
       "      <th>arousal_response</th>\n",
       "      <th>dominance_response</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NOT_SARCASM</td>\n",
       "      <td>0.134328</td>\n",
       "      <td>0.104478</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.074627</td>\n",
       "      <td>4.567164</td>\n",
       "      <td>0.068167</td>\n",
       "      <td>0.283582</td>\n",
       "      <td>0.074627</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.71005</td>\n",
       "      <td>My 3 year old , that just finished readi...</td>\n",
       "      <td>joy</td>\n",
       "      <td>5.671111</td>\n",
       "      <td>4.181667</td>\n",
       "      <td>5.976667</td>\n",
       "      <td>['My', '3', 'year', 'old', ',', 'that', 'just'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SARCASM</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>4.818182</td>\n",
       "      <td>0.219008</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.60115</td>\n",
       "      <td>How many verifiable lies has he told now ?...</td>\n",
       "      <td>joy</td>\n",
       "      <td>5.442857</td>\n",
       "      <td>3.635714</td>\n",
       "      <td>5.554286</td>\n",
       "      <td>['How', 'many', 'verifiable', 'lies', 'has', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SARCASM</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>4.695652</td>\n",
       "      <td>0.204159</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>Maybe Docs just a scrub of a coach ... I...</td>\n",
       "      <td>anger</td>\n",
       "      <td>5.236250</td>\n",
       "      <td>4.210000</td>\n",
       "      <td>5.270000</td>\n",
       "      <td>['Maybe', 'Docs', 'just', 'a', 'scrub', 'of', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NOT_SARCASM</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.272727</td>\n",
       "      <td>0.194215</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.19430</td>\n",
       "      <td>is just a cover up for the real hate insid...</td>\n",
       "      <td>anger</td>\n",
       "      <td>5.174000</td>\n",
       "      <td>4.290000</td>\n",
       "      <td>5.474000</td>\n",
       "      <td>['is', 'just', 'a', 'cover', 'up', 'for', 'the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NOT_SARCASM</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.47420</td>\n",
       "      <td>The irony being that he even has to ask ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>5.795000</td>\n",
       "      <td>3.630000</td>\n",
       "      <td>5.697500</td>\n",
       "      <td>['The', 'irony', 'being', 'that', 'he', 'even'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time  relativ  funct  pronoun  ipron  adverb  cogmech  excl  leisure  conj  \\\n",
       "0   6.0      6.0   19.0      9.0    5.0     3.0      9.0   2.0      2.0   5.0   \n",
       "1   1.0      1.0    6.0      1.0    0.0     1.0      2.0   0.0      0.0   0.0   \n",
       "2   0.0      0.0    7.0      1.0    1.0     1.0      4.0   1.0      2.0   0.0   \n",
       "3   0.0      4.0    9.0      0.0    0.0     1.0      3.0   1.0      0.0   0.0   \n",
       "4   0.0      0.0    6.0      2.0    1.0     1.0      1.0   0.0      0.0   0.0   \n",
       "\n",
       "   incl  verb  past  social  ppron    i  cause  humans  certain  achieve  \\\n",
       "0   2.0   2.0   1.0     7.0    4.0  1.0    3.0     1.0      1.0      4.0   \n",
       "1   0.0   2.0   1.0     2.0    1.0  0.0    0.0     0.0      2.0      0.0   \n",
       "2   1.0   2.0   0.0     1.0    0.0  0.0    0.0     0.0      0.0      1.0   \n",
       "3   1.0   2.0   0.0     0.0    0.0  0.0    0.0     0.0      1.0      0.0   \n",
       "4   0.0   3.0   0.0     2.0    1.0  0.0    1.0     0.0      0.0      0.0   \n",
       "\n",
       "   preps  tentat  space  affect  filler  posemo  present  they  shehe  negemo  \\\n",
       "0    3.0     1.0    1.0     4.0     1.0     3.0      1.0   2.0    1.0     1.0   \n",
       "1    1.0     0.0    0.0     3.0     0.0     2.0      1.0   0.0    1.0     1.0   \n",
       "2    3.0     0.0    0.0     0.0     0.0     0.0      2.0   0.0    0.0     0.0   \n",
       "3    4.0     0.0    4.0     1.0     0.0     0.0      2.0   0.0    0.0     1.0   \n",
       "4    1.0     0.0    0.0     0.0     0.0     0.0      2.0   0.0    1.0     0.0   \n",
       "\n",
       "   anger  quant  auxverb  article  insight  work  you  motion  discrep  \\\n",
       "0    1.0    0.0      0.0      0.0      0.0   0.0  0.0     0.0      0.0   \n",
       "1    1.0    1.0      1.0      1.0      0.0   0.0  0.0     0.0      0.0   \n",
       "2    0.0    0.0      0.0      2.0      1.0   1.0  0.0     0.0      0.0   \n",
       "3    1.0    0.0      1.0      3.0      0.0   0.0  0.0     0.0      0.0   \n",
       "4    0.0    0.0      2.0      0.0      0.0   0.0  0.0     0.0      0.0   \n",
       "\n",
       "   assent  inhib  home  percept  hear  anx  sad  see  money  negate  bio  \\\n",
       "0     0.0    0.0   0.0      0.0   0.0  0.0  0.0  0.0    0.0     0.0  0.0   \n",
       "1     0.0    0.0   0.0      0.0   0.0  0.0  0.0  0.0    0.0     0.0  0.0   \n",
       "2     0.0    0.0   0.0      0.0   0.0  0.0  0.0  0.0    0.0     0.0  0.0   \n",
       "3     0.0    0.0   0.0      0.0   0.0  0.0  0.0  0.0    0.0     0.0  0.0   \n",
       "4     0.0    0.0   0.0      0.0   0.0  0.0  0.0  0.0    0.0     0.0  0.0   \n",
       "\n",
       "   health  sexual  nonfl  future  swear  ingest  feel  number  body  relig  \\\n",
       "0     0.0     0.0    0.0     0.0    0.0     0.0   0.0     0.0   0.0    0.0   \n",
       "1     0.0     0.0    0.0     0.0    0.0     0.0   0.0     0.0   0.0    0.0   \n",
       "2     0.0     0.0    0.0     0.0    0.0     0.0   0.0     0.0   0.0    0.0   \n",
       "3     0.0     0.0    0.0     0.0    0.0     0.0   0.0     0.0   0.0    0.0   \n",
       "4     0.0     0.0    0.0     0.0    0.0     0.0   0.0     0.0   0.0    0.0   \n",
       "\n",
       "   family   we  death  friend        label  noun_count_percent  \\\n",
       "0     0.0  0.0    0.0     0.0  NOT_SARCASM            0.134328   \n",
       "1     0.0  0.0    0.0     0.0      SARCASM            0.181818   \n",
       "2     0.0  0.0    0.0     0.0      SARCASM            0.130435   \n",
       "3     0.0  0.0    0.0     0.0  NOT_SARCASM            0.045455   \n",
       "4     0.0  0.0    0.0     0.0  NOT_SARCASM            0.200000   \n",
       "\n",
       "   adj_count_percent  adv_count_percent  pro_count_percent  char_count  \\\n",
       "0           0.104478           0.089552           0.074627    4.567164   \n",
       "1           0.136364           0.045455           0.090909    4.818182   \n",
       "2           0.043478           0.043478           0.043478    4.695652   \n",
       "3           0.090909           0.045455           0.000000    4.272727   \n",
       "4           0.000000           0.133333           0.066667    4.200000   \n",
       "\n",
       "   word_density  punctuation_count  upper_case_word_count  vader_pos  \\\n",
       "0      0.068167           0.283582               0.074627      0.147   \n",
       "1      0.219008           0.363636               0.090909      0.215   \n",
       "2      0.204159           0.260870               0.173913      0.000   \n",
       "3      0.194215           0.318182               0.181818      0.000   \n",
       "4      0.280000           0.266667               0.200000      0.000   \n",
       "\n",
       "   vader_neg  vader_neu  vader_compound  \\\n",
       "0      0.134      0.719         0.71005   \n",
       "1      0.131      0.654         0.60115   \n",
       "2      0.000      1.000         0.50000   \n",
       "3      0.200      0.800         0.19430   \n",
       "4      0.091      0.909         0.47420   \n",
       "\n",
       "                                            response Response_emotion  \\\n",
       "0        My 3 year old , that just finished readi...              joy   \n",
       "1      How many verifiable lies has he told now ?...              joy   \n",
       "2        Maybe Docs just a scrub of a coach ... I...            anger   \n",
       "3      is just a cover up for the real hate insid...            anger   \n",
       "4        The irony being that he even has to ask ...            anger   \n",
       "\n",
       "   valence_response  arousal_response  dominance_response  \\\n",
       "0          5.671111          4.181667            5.976667   \n",
       "1          5.442857          3.635714            5.554286   \n",
       "2          5.236250          4.210000            5.270000   \n",
       "3          5.174000          4.290000            5.474000   \n",
       "4          5.795000          3.630000            5.697500   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  ['My', '3', 'year', 'old', ',', 'that', 'just'...  \n",
       "1  ['How', 'many', 'verifiable', 'lies', 'has', '...  \n",
       "2  ['Maybe', 'Docs', 'just', 'a', 'scrub', 'of', ...  \n",
       "3  ['is', 'just', 'a', 'cover', 'up', 'for', 'the...  \n",
       "4  ['The', 'irony', 'being', 'that', 'he', 'even'...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TODO\n",
    "\n",
    "csv_file = '/Users/swcam/Documents/GitHub/Sarcasm/Final_data/liwc_response_test.csv'\n",
    "#csv_file = '/Users/swcam/Documents/GitHub/Sarcasm/Final_Features_test_data'\n",
    "Test_df = pd.read_csv(csv_file)\n",
    "Test_df = Test_df.loc[:, ~Test_df.columns.str.contains('^Unnamed')]\n",
    "# print out the first few rows of data info\n",
    "Test_df.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 83)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Train_df\n",
    "\n",
    "X_test= Test_df\n",
    "\n",
    "y_train= Train_df['label']\n",
    "\n",
    "y_test = Test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import preprocessing\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}') # word that appears 1 or more times\n",
    "count_vect.fit(Train_df['response'])\n",
    "count_vect.fit(Test_df['response'])\n",
    "# to show resulting vocabulary; the numbers are not counts, they are the position in the sparse vector.\n",
    "count_vect.vocabulary_\n",
    "\n",
    "# transform the training and validation data using count vectorizer object: doc x term\n",
    "xtrain_count =  count_vect.transform(Train_df['response']) \n",
    "xvalid_count =  count_vect.transform(Test_df['response'])\n",
    "\n",
    "xvalid_count= xvalid_count.toarray()\n",
    "xtrain_count= xtrain_count.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word level tfidf Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000) # considers the top 5000 most frequent features\n",
    "tfidf_vect.fit(Train_df['response'])\n",
    "tfidf_vect.fit(Test_df['response'])\n",
    "\n",
    "\n",
    "tfidf_train = tfidf_vect.transform(Train_df['response'])\n",
    "tfidf_test = tfidf_vect.transform(Test_df['response'])\n",
    "\n",
    "tfidf_train = tfidf_train.toarray()\n",
    "tfidf_test = tfidf_test.toarray()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ngram level tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(Train_df['response'])\n",
    "tfidf_vect_ngram.fit(Test_df['response'])\n",
    "\n",
    "tfidf_ngram_train =  tfidf_vect_ngram.transform(Train_df['response'])\n",
    "tfidf_ngram_test =  tfidf_vect_ngram.transform(Test_df['response'])\n",
    "\n",
    "tfidf_ngram_train = tfidf_ngram_train.toarray()\n",
    "tfidf_ngram_test = tfidf_ngram_test.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character level tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swcam\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "C:\\Users\\swcam\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "  ### characters level tf-idf\n",
    "\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(Train_df['response'])\n",
    "tfidf_vect_ngram_chars.fit(Test_df['response'])\n",
    "\n",
    "\n",
    "tfidf_ngram_chars_train =  tfidf_vect_ngram_chars.transform(Train_df['response']) \n",
    "\n",
    "tfidf_ngram_chars_test =  tfidf_vect_ngram_chars.transform(Test_df['response'])\n",
    "                                   \n",
    "\n",
    "                                   \n",
    "tfidf_ngram_chars_train =  tfidf_ngram_chars_train.toarray()\n",
    "\n",
    "tfidf_ngram_chars_test = tfidf_ngram_chars_test.toarray()\n",
    "                                           \n",
    "                                   \n",
    "                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "# fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "    return metrics.accuracy_score(predictions, y_test),metrics.precision_score(predictions, y_test),metrics.recall_score(predictions, y_test),metrics.f1_score(predictions, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on Test data running only count vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word level tfidf feature RF :  A: 0.58 P: 0.82 R: 0.55 F1: 0.66\n",
      "Count vector feature RF, :  A: 0.62 P: 0.71 R: 0.6 F1: 0.65\n",
      "ngram word level tfidf feature RF, :  A: 0.58 P: 0.82 R: 0.55 F1: 0.66\n",
      "ngram char level tfidf feature RF, :  A: 0.64 P: 0.74 R: 0.61 F1: 0.67\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "accuracy, precision, recall,f1  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), tfidf_ngram_train , y_train, tfidf_ngram_test)\n",
    "print (\"word level tfidf feature RF : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n",
    "accuracy, precision, recall,f1  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), xtrain_count , y_train, xvalid_count)\n",
    "print (\"Count vector feature RF, : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n",
    "accuracy, precision, recall,f1  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), tfidf_ngram_train , y_train, tfidf_ngram_test)\n",
    "print (\"ngram word level tfidf feature RF, : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n",
    "accuracy, precision, recall,f1  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), tfidf_ngram_chars_train , y_train, tfidf_ngram_chars_test)\n",
    "print (\"ngram char level tfidf feature RF, : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_count_features_train = np.column_stack([xtrain_count,tfidf_ngram_train,tfidf_ngram_train,tfidf_ngram_chars_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_count_features_test = np.column_stack([tfidf_ngram_test,xvalid_count,tfidf_ngram_test,tfidf_ngram_chars_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing only count vector combination "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all count features combo RF :  A: 0.63 P: 0.72 R: 0.61 F1: 0.66\n",
      " ngram char level tfidf feature NB, :  A: 0.51 P: 0.65 R: 0.51 F1: 0.57\n",
      "all count features combo SVM :  A: 0.55 P: 0.58 R: 0.55 F1: 0.57\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall,f1  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), all_count_features_train, y_train, all_count_features_test)\n",
    "print (\"all count features combo RF : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n",
    "accuracy, precision, recall,f1  = train_model(naive_bayes.GaussianNB(), all_count_features_train , y_train, all_count_features_test)\n",
    "print (\" all count features combo NB, : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n",
    "accuracy, precision, recall,f1  = train_model(svm.SVC(), all_count_features_train , y_train, all_count_features_test)\n",
    "print (\"all count features combo SVM : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Linguistic Features to Count Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop(['tokenized_text','label','response'], axis=1)\n",
    "X_train = X_train.drop(['tokenized_text','label','response'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['Response_emotion'] = X_test['Response_emotion'].astype('category').cat.codes\n",
    "X_train['Response_emotion'] = X_train['Response_emotion'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL Linguistic Features + ALL count features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_train = np.column_stack([all_count_features_train, X_train])\n",
    "all_features_test = np.column_stack([all_count_features_test, X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linguistic + count features RF :  A: 0.65 P: 0.7 R: 0.64 F1: 0.67\n",
      "Linguistic + count features NB, :  A: 0.6 P: 0.7 R: 0.59 F1: 0.64\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall,f1  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), all_features_train, y_train, all_features_test)\n",
    "print (\"Linguistic + count features RF : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Linguistic Features + Ngram Character tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ling_ngram_chars_train = np.column_stack([tfidf_ngram_chars_train, X_train])\n",
    "all_ling_ngram_chars_test = np.column_stack([tfidf_ngram_chars_test, X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linguistic + count features RF :  A: 0.65 P: 0.7 R: 0.64 F1: 0.67\n",
      "Linguistic + count features NB, :  A: 0.6 P: 0.7 R: 0.59 F1: 0.64\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall,f1  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), all_ling_ngram_chars_train, y_train, all_ling_ngram_chars_test)\n",
    "print (\"all Linguistic Features + Ngram Char level tfidfRF : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n",
    "accuracy, precision, recall,f1  = train_model(naive_bayes.GaussianNB(), all_ling_ngram_chars_train , y_train, all_ling_ngram_chars_test)\n",
    "print (\"all Linguistic Features + Ngram Char level tfidf NB, : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verb</th>\n",
       "      <th>funct</th>\n",
       "      <th>auxverb</th>\n",
       "      <th>present</th>\n",
       "      <th>pronoun</th>\n",
       "      <th>ipron</th>\n",
       "      <th>cogmech</th>\n",
       "      <th>certain</th>\n",
       "      <th>ppron</th>\n",
       "      <th>you</th>\n",
       "      <th>social</th>\n",
       "      <th>affect</th>\n",
       "      <th>posemo</th>\n",
       "      <th>conj</th>\n",
       "      <th>tentat</th>\n",
       "      <th>excl</th>\n",
       "      <th>future</th>\n",
       "      <th>discrep</th>\n",
       "      <th>past</th>\n",
       "      <th>motion</th>\n",
       "      <th>relativ</th>\n",
       "      <th>space</th>\n",
       "      <th>preps</th>\n",
       "      <th>incl</th>\n",
       "      <th>adverb</th>\n",
       "      <th>insight</th>\n",
       "      <th>shehe</th>\n",
       "      <th>achieve</th>\n",
       "      <th>negemo</th>\n",
       "      <th>anger</th>\n",
       "      <th>they</th>\n",
       "      <th>cause</th>\n",
       "      <th>article</th>\n",
       "      <th>money</th>\n",
       "      <th>work</th>\n",
       "      <th>percept</th>\n",
       "      <th>hear</th>\n",
       "      <th>swear</th>\n",
       "      <th>time</th>\n",
       "      <th>negate</th>\n",
       "      <th>leisure</th>\n",
       "      <th>death</th>\n",
       "      <th>number</th>\n",
       "      <th>see</th>\n",
       "      <th>sad</th>\n",
       "      <th>quant</th>\n",
       "      <th>anx</th>\n",
       "      <th>assent</th>\n",
       "      <th>bio</th>\n",
       "      <th>health</th>\n",
       "      <th>filler</th>\n",
       "      <th>body</th>\n",
       "      <th>friend</th>\n",
       "      <th>sexual</th>\n",
       "      <th>we</th>\n",
       "      <th>home</th>\n",
       "      <th>feel</th>\n",
       "      <th>inhib</th>\n",
       "      <th>humans</th>\n",
       "      <th>i</th>\n",
       "      <th>family</th>\n",
       "      <th>relig</th>\n",
       "      <th>nonfl</th>\n",
       "      <th>ingest</th>\n",
       "      <th>label</th>\n",
       "      <th>noun_count_percent</th>\n",
       "      <th>adj_count_percent</th>\n",
       "      <th>adv_count_percent</th>\n",
       "      <th>pro_count_percent</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>upper_case_word_count</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>vader_neu</th>\n",
       "      <th>vader_compound</th>\n",
       "      <th>response</th>\n",
       "      <th>Response_emotion</th>\n",
       "      <th>valence_response</th>\n",
       "      <th>arousal_response</th>\n",
       "      <th>dominance_response</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SARCASM</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>4.862069</td>\n",
       "      <td>0.167658</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.87530</td>\n",
       "      <td>I don't get this .. obviously you do car...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>6.222500</td>\n",
       "      <td>3.695000</td>\n",
       "      <td>5.965000</td>\n",
       "      <td>['I', 'do', \"n't\", 'get', 'this', '..', 'obvio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SARCASM</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.11985</td>\n",
       "      <td>trying to protest about . Talking about hi...</td>\n",
       "      <td>anger</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>3.713333</td>\n",
       "      <td>5.733333</td>\n",
       "      <td>['trying', 'to', 'protest', 'about', '.', 'Tal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SARCASM</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>5.777778</td>\n",
       "      <td>0.320988</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.27130</td>\n",
       "      <td>He makes an insane about of money from t...</td>\n",
       "      <td>anger</td>\n",
       "      <td>5.246667</td>\n",
       "      <td>5.223333</td>\n",
       "      <td>5.426667</td>\n",
       "      <td>['He', 'makes', 'an', 'insane', 'about', 'of',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SARCASM</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>0.243056</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.24470</td>\n",
       "      <td>Meanwhile Trump won't even release his SAT...</td>\n",
       "      <td>anger</td>\n",
       "      <td>5.661111</td>\n",
       "      <td>4.428889</td>\n",
       "      <td>5.907778</td>\n",
       "      <td>['Meanwhile', 'Trump', 'wo', \"n't\", 'even', 'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SARCASM</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>5.575758</td>\n",
       "      <td>0.168962</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.83525</td>\n",
       "      <td>Pretty Sure the Anti-Lincoln Crowd Claimed...</td>\n",
       "      <td>joy</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>['Pretty', 'Sure', 'the', 'Anti-Lincoln', 'Cro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   verb  funct  auxverb  present  pronoun  ipron  cogmech  certain  ppron  \\\n",
       "0     7     13        3        5        5      1        6        1      4   \n",
       "1     2     11        1        2        5      1        3        0      4   \n",
       "2     1      5        0        1        0      0        1        0      0   \n",
       "3     3      9        1        0        4      0        2        1      4   \n",
       "4     0      5        0        0        0      0        0        0      0   \n",
       "\n",
       "   you  social  affect  posemo  conj  tentat  excl  future  discrep  past  \\\n",
       "0    3       4       2       2     2       1     1       1        1     1   \n",
       "1    0       4       1       0     2       0     0       0        0     0   \n",
       "2    0       0       0       0     0       0     0       0        0     0   \n",
       "3    0       5       1       0     1       0     0       0        0     3   \n",
       "4    0       0       0       0     0       0     0       0        0     0   \n",
       "\n",
       "   motion  relativ  space  preps  incl  adverb  insight  shehe  achieve  \\\n",
       "0       1        2      1      2     2       1        1      1        0   \n",
       "1       0        0      0      3     2       2        0      2        1   \n",
       "2       0        0      0      3     0       1        0      0        0   \n",
       "3       0        1      0      0     1       2        0      3        0   \n",
       "4       0        2      2      2     0       1        0      0        0   \n",
       "\n",
       "   negemo  anger  they  cause  article  money  work  percept  hear  swear  \\\n",
       "0       0      0     0      0        0      0     0        0     0      0   \n",
       "1       1      1     2      1        0      0     0        0     0      0   \n",
       "2       0      0     0      1        2      1     0        0     0      0   \n",
       "3       1      1     1      0        1      0     3        1     1      1   \n",
       "4       0      0     0      0        2      0     0        0     0      0   \n",
       "\n",
       "   time  negate  leisure  death  number  see  sad  quant  anx  assent  bio  \\\n",
       "0     0       0        0      0       0    0    0      0    0       0    0   \n",
       "1     0       0        0      0       0    0    0      0    0       0    0   \n",
       "2     0       0        0      0       0    0    0      0    0       0    0   \n",
       "3     1       0        0      0       0    0    0      0    0       0    0   \n",
       "4     0       0        0      0       0    0    0      0    0       0    0   \n",
       "\n",
       "   health  filler  body  friend  sexual  we  home  feel  inhib  humans  i  \\\n",
       "0       0       0     0       0       0   0     0     0      0       0  0   \n",
       "1       0       0     0       0       0   0     0     0      0       0  0   \n",
       "2       0       0     0       0       0   0     0     0      0       0  0   \n",
       "3       0       0     0       0       0   0     0     0      0       0  0   \n",
       "4       0       0     0       0       0   0     0     0      0       0  0   \n",
       "\n",
       "   family  relig  nonfl  ingest    label  noun_count_percent  \\\n",
       "0       0      0      0       0  SARCASM            0.310345   \n",
       "1       0      0      0       0  SARCASM            0.250000   \n",
       "2       0      0      0       0  SARCASM            0.055556   \n",
       "3       0      0      0       0  SARCASM            0.208333   \n",
       "4       0      0      0       0  SARCASM            0.060606   \n",
       "\n",
       "   adj_count_percent  adv_count_percent  pro_count_percent  char_count  \\\n",
       "0           0.034483           0.172414           0.172414    4.862069   \n",
       "1           0.000000           0.000000           0.166667    5.000000   \n",
       "2           0.000000           0.000000           0.055556    5.777778   \n",
       "3           0.041667           0.166667           0.166667    5.833333   \n",
       "4           0.030303           0.030303           0.030303    5.575758   \n",
       "\n",
       "   word_density  punctuation_count  upper_case_word_count  vader_pos  \\\n",
       "0      0.167658           0.379310               0.137931      0.204   \n",
       "1      0.208333           0.166667               0.125000      0.000   \n",
       "2      0.320988           0.333333               0.222222      0.000   \n",
       "3      0.243056           0.166667               0.125000      0.000   \n",
       "4      0.168962           0.393939               0.090909      0.193   \n",
       "\n",
       "   vader_neg  vader_neu  vader_compound  \\\n",
       "0      0.000      0.796         0.87530   \n",
       "1      0.256      0.744         0.11985   \n",
       "2      0.176      0.824         0.27130   \n",
       "3      0.130      0.870         0.24470   \n",
       "4      0.000      0.807         0.83525   \n",
       "\n",
       "                                            response Response_emotion  \\\n",
       "0        I don't get this .. obviously you do car...          sadness   \n",
       "1      trying to protest about . Talking about hi...            anger   \n",
       "2        He makes an insane about of money from t...            anger   \n",
       "3      Meanwhile Trump won't even release his SAT...            anger   \n",
       "4      Pretty Sure the Anti-Lincoln Crowd Claimed...              joy   \n",
       "\n",
       "   valence_response  arousal_response  dominance_response  \\\n",
       "0          6.222500          3.695000            5.965000   \n",
       "1          5.190000          3.713333            5.733333   \n",
       "2          5.246667          5.223333            5.426667   \n",
       "3          5.661111          4.428889            5.907778   \n",
       "4          5.000000          5.000000            5.000000   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  ['I', 'do', \"n't\", 'get', 'this', '..', 'obvio...  \n",
       "1  ['trying', 'to', 'protest', 'about', '.', 'Tal...  \n",
       "2  ['He', 'makes', 'an', 'insane', 'about', 'of',...  \n",
       "3  ['Meanwhile', 'Trump', 'wo', \"n't\", 'even', 'r...  \n",
       "4  ['Pretty', 'Sure', 'the', 'Anti-Lincoln', 'Cro...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Linguistic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAP\n",
    "\n",
    "Train_df_select = Train_df[['vader_compound', 'valence_response', 'time','word_density','noun_count_percent','dominance_response','you','upper_case_word_count','adj_count_percent','char_count','vader_neu','arousal_response','funct','pro_count_percent','Response_emotion','ppron','relativ','adv_count_percent']]\n",
    "Test_df_select = Test_df[['vader_compound', 'valence_response', 'time','word_density','noun_count_percent','dominance_response','you','upper_case_word_count','adj_count_percent','char_count','vader_neu','arousal_response','funct','pro_count_percent','Response_emotion','ppron','relativ','adv_count_percent']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_df_select ['Response_emotion'] = Train_df_select ['Response_emotion'].astype('category').cat.codes\n",
    "Test_df_select['Response_emotion'] = Test_df_select['Response_emotion'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Linguistic Features RF :  A: 0.59 P: 0.55 R: 0.6 F1: 0.57\n",
      "Select Linguistic Features NB, :  A: 0.59 P: 0.61 R: 0.59 F1: 0.6\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall,f1  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), Train_df_select, y_train, Test_df_select)\n",
    "print (\"Select Linguistic Features RF : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Linguistic Features + All counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_ling_all_count_train = np.column_stack([all_count_features_train, Train_df_select])\n",
    "select_ling_all_count_test = np.column_stack([all_count_features_test, Test_df_select])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Linguistic + ALL counts RF :  A: 0.65 P: 0.73 R: 0.63 F1: 0.68\n",
      "Select Linguistic + ALL counts NB, :  A: 0.51 P: 0.65 R: 0.51 F1: 0.57\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall,f1  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), select_ling_all_count_train, y_train, select_ling_all_count_test)\n",
    "print (\"Select Linguistic + ALL counts RF : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Linguistic Features + ngram char tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_ling_ngram_char_train = np.column_stack([tfidf_ngram_chars_train, Train_df_select])\n",
    "select_ling_ngram_char_test = np.column_stack([tfidf_ngram_chars_test, Test_df_select])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Linguistic + tfidf ngram char RF :  A: 0.64 P: 0.68 R: 0.63 F1: 0.65\n",
      "Select Linguistic + tfidf ngram char NB, :  A: 0.6 P: 0.7 R: 0.58 F1: 0.64\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall,f1  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), select_ling_ngram_char_train, y_train, select_ling_ngram_char_test)\n",
    "print (\"Select Linguistic + tfidf ngram char RF : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n",
    "accuracy, precision, recall,f1  = train_model(naive_bayes.GaussianNB(), select_ling_ngram_char_train, y_train, select_ling_ngram_char_test)\n",
    "print (\"Select Linguistic + tfidf ngram char NB, : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
